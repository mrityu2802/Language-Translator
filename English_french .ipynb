{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2083a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37596656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e5862c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>french</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tom found that</td>\n",
       "      <td>Tom a trouvé ça</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>begin</td>\n",
       "      <td>Commence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we all wished for peace</td>\n",
       "      <td>Nous souhaitions toutes la paix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i already called him</td>\n",
       "      <td>Je lai déjà appelé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>youre very resourceful</td>\n",
       "      <td>Vous êtes pleines de ressources</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       eng                           french\n",
       "0           tom found that                  Tom a trouvé ça\n",
       "1                    begin                         Commence\n",
       "2  we all wished for peace  Nous souhaitions toutes la paix\n",
       "3     i already called him               Je lai déjà appelé\n",
       "4   youre very resourceful  Vous êtes pleines de ressources"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be65aa3",
   "metadata": {},
   "source": [
    "# Prepare Input data for the Encoder :- the input data to encoder which are preprocessed by following steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05dd95c",
   "metadata": {},
   "source": [
    "Tokenize the English Sentences from eng.\n",
    "Determine the maximum length of English sentence it will be used for padding.\n",
    "Determine the Vocabulary size for english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b1d53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English max length is 47\n",
      "Input data Shape : (8000, 47)\n",
      "No.of Encoder tokens : 4328\n"
     ]
    }
   ],
   "source": [
    "eng_lines = list()\n",
    "for eng_data in df.eng:\n",
    "    eng_lines.append(eng_data)\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(eng_lines)\n",
    "tokenized_eng_lines = tokenizer.texts_to_sequences(eng_lines)\n",
    "\n",
    "length_list = []\n",
    "for token_seq in tokenized_eng_lines:\n",
    "    length_list.append(len(token_seq))\n",
    "    \n",
    "max_input_length = np.array(length_list).max()\n",
    "print(f\"English max length is {format(max_input_length)}\")\n",
    "\n",
    "padded_eng_lines = preprocessing.sequence.pad_sequences(tokenized_eng_lines,maxlen = max_input_length,padding='post')\n",
    "encoder_input_data = np.array(padded_eng_lines)\n",
    "print(f\"Input data Shape : {format(encoder_input_data.shape)}\")\n",
    "\n",
    "eng_word_dict = tokenizer.word_index\n",
    "num_eng_tokens = len(eng_word_dict) + 1\n",
    "print(f\"No.of Encoder tokens : {format(num_eng_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057275e",
   "metadata": {},
   "source": [
    "# Prepare Input data for Decoder\n",
    "The decoder model will be fed with processed french lines. The steps are same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "207e0059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French max length is 53\n",
      "Output data Shape : (8000, 53)\n",
      "Number of Decoder tokens : 6850\n"
     ]
    }
   ],
   "source": [
    "french_lines = []\n",
    "for french_data in df.french:\n",
    "    french_lines.append('_START_ '+french_data+'_END_')\n",
    "    \n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( french_lines ) \n",
    "tokenized_french_lines = tokenizer.texts_to_sequences( french_lines ) \n",
    "\n",
    "length_list = []\n",
    "for token_seq in tokenized_french_lines:\n",
    "    length_list.append( len( token_seq ))\n",
    "max_output_length = np.array( length_list ).max()\n",
    "print( f\"French max length is {format( max_output_length )}\")\n",
    "\n",
    "padded_french_lines = preprocessing.sequence.pad_sequences( tokenized_french_lines , maxlen=max_output_length, padding='post' )\n",
    "decoder_input_data = np.array( padded_french_lines )\n",
    "print( f\"Output data Shape : {format( decoder_input_data.shape )}\")\n",
    "\n",
    "french_word_dict = tokenizer.word_index\n",
    "num_french_tokens = len( french_word_dict )+1\n",
    "print( f\"Number of Decoder tokens : {format( num_french_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189f225f",
   "metadata": {},
   "source": [
    "# Prepare Target Data for decoder we modify it like:-\n",
    "We remove start token\n",
    "Convert the padded french lines *One Possible result may look like:- ['START','Some random text','END'] -> ['Some random text','END']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb9121cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder target data shape -> (8000, 53, 6850)\n"
     ]
    }
   ],
   "source": [
    "decoder_target_data = []\n",
    "for token_seq in tokenized_french_lines:\n",
    "    decoder_target_data.append(token_seq[1:])\n",
    "    \n",
    "padded_french_lines = preprocessing.sequence.pad_sequences(decoder_target_data , maxlen=max_output_length, padding='post' )\n",
    "onehot_french_lines = utils.to_categorical( padded_french_lines , num_french_tokens )\n",
    "decoder_target_data = np.array( onehot_french_lines )\n",
    "print( f\"Decoder target data shape -> {format( decoder_target_data.shape )}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879fc6f",
   "metadata": {},
   "source": [
    "# Lets Define Our Encoder-Decoder Model\n",
    "The model is LSTM based and configuration follows as:-\n",
    "\n",
    "No of input layers -> 2(one for encoded input data and another for decoded)\n",
    "\n",
    "Embedding layer will be used for fix sized dense vectors.\n",
    "\n",
    "LSTM will be used as mentioned above\n",
    "\n",
    "Working :-\n",
    "\n",
    "encoder input data -> Embedding layer\n",
    "\n",
    "Output of Embedding layer goes to LSTM which produces 2 state h and c which are encoded states\n",
    "\n",
    "These states (h and c) are set in the LSTM cell of the decoder\n",
    "\n",
    "The decoder input data comes in through Embedding layer\n",
    "\n",
    "The Embedding goes in LSTM to produce sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66af699c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 256)    1107968     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    1753600     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 128), (None, 197120      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 128),  197120      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 6850)   883650      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,139,458\n",
      "Trainable params: 4,139,458\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( num_eng_tokens, 256 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 128 , return_state=True  )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( num_french_tokens, 256 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 128 , return_state=True , return_sequences=True)\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( num_french_tokens , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a3610",
   "metadata": {},
   "source": [
    " Now Let us Train our Model with 50 epochs and RMSprop optimizer and categorical crossentropy loss function.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "208655c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "32/32 [==============================] - 91s 2s/step - loss: 1.0530\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - 80s 3s/step - loss: 0.8926\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - 80s 3s/step - loss: 0.8496\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - 74s 2s/step - loss: 0.8111\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - 66s 2s/step - loss: 0.7807\n",
      "Epoch 6/40\n",
      "32/32 [==============================] - 65s 2s/step - loss: 0.7574\n",
      "Epoch 7/40\n",
      "32/32 [==============================] - 66s 2s/step - loss: 0.7369\n",
      "Epoch 8/40\n",
      "32/32 [==============================] - 65s 2s/step - loss: 0.7190\n",
      "Epoch 9/40\n",
      "32/32 [==============================] - 67s 2s/step - loss: 0.7014\n",
      "Epoch 10/40\n",
      "32/32 [==============================] - 65s 2s/step - loss: 0.6841\n",
      "Epoch 11/40\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.6671\n",
      "Epoch 12/40\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.6509\n",
      "Epoch 13/40\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.6350\n",
      "Epoch 14/40\n",
      "32/32 [==============================] - 65s 2s/step - loss: 0.6199\n",
      "Epoch 15/40\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.6057\n",
      "Epoch 16/40\n",
      "32/32 [==============================] - 65s 2s/step - loss: 0.5925\n",
      "Epoch 17/40\n",
      "32/32 [==============================] - 73s 2s/step - loss: 0.5795\n",
      "Epoch 18/40\n",
      "32/32 [==============================] - 77s 2s/step - loss: 0.5673\n",
      "Epoch 19/40\n",
      "32/32 [==============================] - 76s 2s/step - loss: 0.5555\n",
      "Epoch 20/40\n",
      "32/32 [==============================] - 75s 2s/step - loss: 0.5438\n",
      "Epoch 21/40\n",
      "32/32 [==============================] - 75s 2s/step - loss: 0.5326\n",
      "Epoch 22/40\n",
      "32/32 [==============================] - 75s 2s/step - loss: 0.5221\n",
      "Epoch 23/40\n",
      "32/32 [==============================] - 72s 2s/step - loss: 0.5111\n",
      "Epoch 24/40\n",
      "32/32 [==============================] - 73s 2s/step - loss: 0.5014\n",
      "Epoch 25/40\n",
      "32/32 [==============================] - 83s 3s/step - loss: 0.4914\n",
      "Epoch 26/40\n",
      "32/32 [==============================] - 79s 2s/step - loss: 0.4821\n",
      "Epoch 27/40\n",
      "32/32 [==============================] - 77s 2s/step - loss: 0.4727\n",
      "Epoch 28/40\n",
      "32/32 [==============================] - 74s 2s/step - loss: 0.4636\n",
      "Epoch 29/40\n",
      "32/32 [==============================] - 68s 2s/step - loss: 0.4545\n",
      "Epoch 30/40\n",
      "32/32 [==============================] - 68s 2s/step - loss: 0.4461\n",
      "Epoch 31/40\n",
      "32/32 [==============================] - 77s 2s/step - loss: 0.4372\n",
      "Epoch 32/40\n",
      "32/32 [==============================] - 72s 2s/step - loss: 0.4290\n",
      "Epoch 33/40\n",
      "32/32 [==============================] - 72s 2s/step - loss: 0.4207\n",
      "Epoch 34/40\n",
      "32/32 [==============================] - 85s 3s/step - loss: 0.4126\n",
      "Epoch 35/40\n",
      "32/32 [==============================] - 74s 2s/step - loss: 0.4051\n",
      "Epoch 36/40\n",
      "32/32 [==============================] - 65s 2s/step - loss: 0.3969\n",
      "Epoch 37/40\n",
      "32/32 [==============================] - 66s 2s/step - loss: 0.3892\n",
      "Epoch 38/40\n",
      "32/32 [==============================] - 65s 2s/step - loss: 0.3815\n",
      "Epoch 39/40\n",
      "32/32 [==============================] - 66s 2s/step - loss: 0.3740\n",
      "Epoch 40/40\n",
      "32/32 [==============================] - 65s 2s/step - loss: 0.3669\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=250, epochs=40 ) \n",
    "model.save( 'model.h5' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b7799",
   "metadata": {},
   "source": [
    "\n",
    "Lets define Inference¶\n",
    "We will create a inference which will help predicting translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9354577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 128 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 128 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12a7d37",
   "metadata": {},
   "source": [
    "# Some Translation¶\n",
    "First we take english sentences and predict the state values using encoder model (see inference)\n",
    "We set the state value in the decoder's LSTM\n",
    "Then we generate a sequence which contain start token\n",
    "We input the sequence in the dec model\n",
    "We replace the start token\n",
    "We carry out the above steps again and again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05c91352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( eng_word_dict[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c606e40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter eng sentence : How are you\n",
      " comment vous end\n",
      "Enter eng sentence : we all wished for peace\n",
      " nous avons une grosse erreur end\n",
      "Enter eng sentence : i called him\n",
      " je lai ai en train end\n",
      "Enter eng sentence : help the poor\n",
      " les dents end\n",
      "Enter eng sentence : god is watching us\n",
      " restez end\n",
      "Enter eng sentence : stay where you are\n",
      " où vous vous end\n",
      "Enter eng sentence : i already called him\n",
      " je lai fait de la maison end\n",
      "Enter eng sentence : end this now\n",
      " cest end\n",
      "Enter eng sentence : bjbjhbjh\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'bjbjhbjh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b5b43e076a32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mstates_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mstr_to_tokens\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'Enter eng sentence : '\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m#states_values = enc_model.predict( encoder_input_data[ epoch ] )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mempty_target_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-122133c8c682>\u001b[0m in \u001b[0;36mstr_to_tokens\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtokens_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mtokens_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0meng_word_dict\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokens_list\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_input_length\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'bjbjhbjh'"
     ]
    }
   ],
   "source": [
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "for epoch in range( encoder_input_data.shape[0] ):\n",
    "    states_values = enc_model.predict( str_to_tokens( input( 'Enter eng sentence : ' ) ) )\n",
    "    #states_values = enc_model.predict( encoder_input_data[ epoch ] )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = french_word_dict['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in french_word_dict.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceef74ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
